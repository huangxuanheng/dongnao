## 百万连接优化

### 测试环境


| 地址 | CPU | 内存 | JDK |
| --- | --- | --- | --- |
| 10.10.1.9 | VM-4核 | 8G | 1.8 |
| 10.10.1.10 | VM-4核 | 8G | 1.8 |

### 一、启动

- 启动服务端
    ```sh
    java -Xmx4g -Xms4g -cp network-study-1.0-SNAPSHOT-jar-with-dependencies.jar com.dongnaoedu.network.netty.million.Server > out.log 2>&1 &
    ```
    
- 启动客户端
    ```sh
    java -Xmx4g -Xms4g -Dserver.host=10.10.1.10 -cp network-study-1.0-SNAPSHOT-jar-with-dependencies.jar com.dongnaoedu.network.netty.million.Client
    ```
    
- 客户端启动后，如果报了以下错误，需要修改系统的文件最大句柄和进程的文件最大句柄。
    ```
    Caused by: java.io.IOException: Too many open files
        at sun.nio.ch.FileDispatcherImpl.init(Native Method)
        at sun.nio.ch.FileDispatcherImpl.<clinit>(FileDispatcherImpl.java:35)
        ... 8 more
    ```

### 二、优化系统最大文件句柄

1. 查看操作系统最大文件句柄数，执行命令`cat /proc/sys/fs/file-max`，查看最大句柄数是否满足需要，如果不满足，通过`vim /etc/sysctl.conf`命令插入如下配置：
```sh
fs.file-max = 1000000
```
配置完成后，执行`sysctl -p`命令，让配置立即生效

2. 设置单进程打开的文件最大句柄数，执行命令`ulimit -a`查看当前设置是否满足要求：
```sh
[root@test-server2 download]# ulimit -a | grep "open files"
open files                      (-n) 1024
```
当并发接入的Tcp连接数超过上限时，就会提示“Too many open files”，所有的新客户端接入将会失败。通过`vim /etc/security/limits.conf` 修改配置参数：
```
* soft nofile 1000000
* hard nofile 1000000
```
修改配置参数后`注销`生效。

- 如果程序被中断，或报了异常
```java
java.io.IOException: 设备上没有空间
	at sun.nio.ch.EPollArrayWrapper.epollCtl(Native Method)
	at sun.nio.ch.EPollArrayWrapper.updateRegistrations(EPollArrayWrapper.java:299)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:268)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	at sun.nio.ch.SelectorImpl.selectNow(SelectorImpl.java:105)
	at io.netty.channel.nio.SelectedSelectionKeySetSelector.selectNow(SelectedSelectionKeySetSelector.java:56)
	at io.netty.channel.nio.NioEventLoop.selectNow(NioEventLoop.java:750)
	at io.netty.channel.nio.NioEventLoop$1.get(NioEventLoop.java:71)
	at io.netty.channel.DefaultSelectStrategy.calculateStrategy(DefaultSelectStrategy.java:30)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:426)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:905)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

```
- 此时可以查看操作系统的日志`more /var/log/messages`，或在程序启动时执行`tail -f /var/log/messages` 监控日志。如果日志中出现以下内容，说明需要优化TCP/IP参数
```
Jun  4 16:55:01 localserver kernel: TCP: too many orphaned sockets
Jun  4 16:55:01 localserver kernel: TCP: too many orphaned sockets
Jun  4 16:55:01 localserver kernel: TCP: too many orphaned sockets
Jun  4 16:55:01 localserver kernel: TCP: too many orphaned sockets
Jun  4 16:55:01 localserver kernel: TCP: too many orphaned sockets
Jun  4 16:55:01 localserver kernel: TCP: too many orphaned sockets
Jun  4 16:55:01 localserver kernel: TCP: too many orphaned sockets
```


### 三、优化TCP/IP相关参数
- 查看客户端端口范围限制
```
cat /proc/sys/net/ipv4/ip_local_port_range
```

- 通过`vim /etc/sysctl.conf` 修改网络参数

- 客户端修改端口范围的限制
```
net.ipv4.ip_local_port_range = 1024 65535
```
- 优化TCP参数

```
net.ipv4.tcp_mem = 786432 2097152 3145728
net.ipv4.tcp_wmem = 4096 4096 16777216
net.ipv4.tcp_rmem = 4096 4096 16777216
net.ipv4.tcp_keepalive_time = 1800
net.ipv4.tcp_keepalive_intvl = 20
net.ipv4.tcp_keepalive_probes = 5
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
```
##### 参数说明

> **net.ipv4.tcp_mem：** 分配给tcp连接的内存，单位是page（1个Page通常是4KB，可以通过`getconf PAGESIZE`命令查看），三个值分别是最小、默认、和最大。比如以上配置中的最大是3145728，那分配给tcp的最大内存=3145728*4 / 1024 / 1024 = 12GB。一个TCP连接大约占7.5KB，粗略可以算出百万连接≈7.5*1000000/4=1875000  3145728足以满足测试所需。

> **net.ipv4.tcp_wmem：** 为每个TCP连接分配的写缓冲区内存大小，单位是字节。三个值分别是最小、默认、和最大。

> **net.ipv4.tcp_rmem：** 为每个TCP连接分配的读缓冲区内存大小，单位是字节。三个值分别是最小、默认、和最大。

> **net.ipv4.tcp_keepalive_time：** 最近一次数据包发送与第一次keep alive探测消息发送的事件间隔，用于确认TCP连接是否有效。

> **net.ipv4.tcp_keepalive_intvl:** 在未获得探测消息响应时，发送探测消息的时间间隔。

> **net.ipv4.tcp_keepalive_probes：** 判断TCP连接失效连续发送的探测消息个数，达到之后判定连接失效。

> **net.ipv4.tcp_tw_reuse：** 是否允许将TIME_WAIT Socket 重新用于新的TCP连接，默认为0，表示关闭。

> **net.ipv4.tcp_tw_recycle：** 是否开启TIME_WAIT Socket 的快速回收功能，默认为0，表示关闭。

> **net.ipv4.tcp_fin_timeout：** 套接字自身关闭时保持在FIN_WAIT_2 状态的时间。默认为60。


