http {
	# 根据每次请求的$http_user_agent，匹配map块中的左侧字符
	# 匹配到的字符串，则将右侧内容赋值给$agent变量
	map $http_user_agent $agent {
		default "";  # 其他的都是爬虫程序身份，未匹配到的为""
		~curl $http_user_agent;
		~*apachebench $http_user_agent;
		~*spider $http_user_agent;
		~*bot $http_user_agent;
		~*slurp $http_user_agent;
	}

	# 根据每次匹配到的爬虫身份为key，定义它们的连接数、请求数
	# 不会为空值""Key建立限制
	limit_conn_zone $agent zone=conn_ttlsa_com:10m;
	limit_req_zone $agent zone=req_ttlsa_com:10m rate=1r/s;

	server {
		listen 80;
		# 目录下放置robots.txt文件
		root /data/www/;
		location / {
			# 启用连接数、请求数的限制
			limit_req zone=conn_ttlsa_com burst=5;
			limit_conn req_ttlsa_com 1;
			limit_rate 500k;
		}
	}
}